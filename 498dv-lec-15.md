Announcements:
- Midterm review next week
- Midterm: open book, open notes, can bring laptops, no browsers/internet

## Network Architecture Search
How do we save memory in terms of training on resources constrained?

**Input**: $1024 \times 2048$
We can downsample to $256 \times 128$

**Conv. Layer**:
- Num. of output channels
- Kernel size
- Stridelength

Note we have lot of choices for model design.

Naive method: 
- Try all the choices, pick one that works best
- Random sampling
	- Only works well if we have limitations and identify a subset

This is where Network Architecture Search (NAS) comes in.

**Typical:**
Human determines Architecture; Data -> training -> weights
**NAS:**
Data -> search -> architecture; More data -> training -> weights
Human still needs to determines primitives (ie. Convolutional followed by fully connected)

## TinyNAS Paper
Insight: More computation leads to better models

...

## Interpreter vs. Code Generation
1. FLOPs vs. Accuracy
2. Engine vs. Accuracy

Bad engine -> not optimized for tiny devices
Search space for NAS decreases

"Not-optimized"
- Best convolution 5x5
Optimized engine
- "More types of networks"

If we have a choice between optimized engine or not, always better to get optimized engine because it searches in subspace that is known to be good, and can only be as good or better.

Generic Optimized engine
- Conv.
- Fully connected
- LSTM units

## Model Adaptive Memory Scheduling
im2col - Linearlizes convolution such that
$M\times N$ kernal becomes array with size $M*N$
and then each kernal is a row in the matrix so you have a $K\times M*N$ array where $K$ is the number of convolutions