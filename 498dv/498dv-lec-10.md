Today:
- Kernal
- Regularization

### Recap: Regression
$x: \mathbb{R}^d$, $y: \mathbb{R}$, $\ell: ||\text{		}||^2$
$f:$ linear functions, $\theta_d$
$f_{\theta}(x) = \theta^Tx$

We want to find $\hat{\theta}$
$\hat{\theta} = argmin_{\theta}\Sigma_{i=0}^{n-1}||f_{\theta}(x_i)-y_i||^2$

We use gradient descent where
$\theta^t = \theta^{t-1} - \alpha\nabla_{\theta}\ell$
Where $\alpha$ is the learning rate.

But this is only linear, ideally we want to learn more complex functions.
Instead of doing regression on $x$, we can do regression on $\phi(x)$, where $\phi$ is some feature set (and any dimensional)

## Kernals
We can think of $\phi$ as literally any combination of functions
ie. $[x, x^2, x^3,...,x^k]$, $[x, log(x), e^x, ...]$

We can take dimensionality of $\phi$ denoted as $K$ where $K\rightarrow \infty$, but how do we map something to infinite dimensional space?

### Matrix Formulation
Consider $y = \theta^Tx$
where $y$ has dimension $n\times1$, $x$ has dimension $n\times d$, and $\theta$ has dimension $1\times d$

$\ell = ||\hat{x}_{n\times d}\theta^T_{d\times 1} - \hat{y}_{n\times 1}||^2$

And we can solve using least squares.
$\theta = (\hat{x}^Tx)^{-1}\hat{x}^T\hat{y} = \hat{x}^T(\hat{x}^Tx)^{-1}\hat{y}$

Question: Why can we just flip $X^T$ to be on the other side?

$\theta = \hat{x}^TC_{n\times1}$ where $C = (\hat{x}^Tx)^{-1}\hat{y}$

$f(x) = \theta^Tx = (\Sigma C_ix_i)^Tx_{d\times 1}$

**... Copy paste some proof here from the lecture notes please**

**Something something kernals**

In order to make sure it is invertible, we can add some "noise" similar to naive bayes' **laplace constant** such that

$\theta = \hat{x}^T(\hat{x}x+\lambda I)\hat{y}$
Where $\lambda \in \mathbb{R^+}$

### Regularization
$\ell = ||\theta^Tx - y||^2$
Think:
We could have a line fit a point, or we can have some n-dimensional polynomial (an infinite number of them in fact) that fits the point.

We like simpler models though, so we instead do:
$\ell = ||\theta^Tx - y||^2 + \lambda ||\theta^T||^2$ and minimize the 2-norm of the theta as well.

The idea of regularization is to reduce overfit.

#### 2-norm vs. 1-norm regularization
1-norm:
- promotes sparsity, aka some values of $\theta$ will be 0
	- Yeah it actually like promotes zero
- If we know a lot of weights will be sparse, we can use the 1-norm

2-norm:
- Simpler functions,
- Not necessarily sparse

0-norm is defined as number of non-zero values of $\theta$. Since 0-norm is non-differentiable, we can use the 1-norm to approximate to promote sparsity.


### Under-fitting vs. Overfitting
Underfitting - Fit a function less complex than our distribution
Overfitting - Fit a function more complex than the true distribution

How do we know if we're underfitting or overfitting? We look at training loss and test loss

We get Training loss continually goes down, but after some point (aka num of batches), the test loss will go from decreasing to increasing.

Tools:
- Regularization (minimize 2-norm, 1-norm)
- Early Stopping

Question: Can't I do optimization on the Test (think euler's whatever)? Is this adaptive learning rate?

Question: What is the difference of using 1-norm to promote sparsity and trimming?
Answer: Basically the same, as the 1-norm promotes zero. Need to check...

#
Next week:
- Decision Trees
- Neural networks