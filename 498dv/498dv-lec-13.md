Announcements:
- PSET 2 is out
- Project Ideas are out
- 2 Reading assignments per week

Today:
- Deep Neural Networks
- Convolutional Networks
- ReLu
- Loss Functions
- Training, Dropout, etc.

## What do you mean by deep?
- Anything more than 2 hidden layers is called "deep"
- One way to look at it mathematically is that this is a composition of function where hidden layers ($f_1...f_k$) learn different type of function (ie. log, cos, x^2)
- Depth helps a lot more (models more complex functions)
- Think of each previous layers as crafting our features that we then use for regression/classification

## Why Now?
1. Datasets (large)
2. Compute

## Convolutional Networks
We take a sliding window multiplied from some weights which we call a "Convolution"

We can take strides for our sliding window, but what happens if we move off the cliff?

"Kernal" $\rightarrow$ 2x2 convolution, stride = 1

We could zero-pad on the side and try and ensure that your output/input is the same size. This also ensure that our input and output stay the same size. We could also try and not go off the edge.

## Receptive Fields
The receptive fields of a node is how many input values it depends on. As we go deeper into the convolution, the receptive fields of nodes at that level will increase. 

At the start, we want to learn about edges and features, and then deeper into the network, learn about more complex things like eyebrow, facial shape, etc.

## ReLu
Classification, we can use sigmoid
Multi-class classification, we can use softmax

Using sigmoid for deeper networks is not necessarily good because it has a small gradient unless it's near the middle, so we use the Rectified Linear Unit (ReLU) instead.

ReLU(x):
- $0$ if $x < 0$
- $x$ if $x >= 0$

Problem is it is non-differentiable at 0. It practice, that's fine because it's just one point. 
If you really wanted to avoid this scenario, you can use the Leaky ReLU

Leak ReLU(x):
- $\alpha x$, if $x < 0$
- $x$ if $x > 0$
where $0 < \alpha << 1$

## Loss Functions
"Cross-Entropy Loss"
Entropy: $H(x) = -\sum p(X=x)log p(X=x)$

Head: $-p(logp)$
Tail: $-(1-p) log(1-p)$

Cross-entropy loss: $-\sum t_i(logp_i)$
$t_i$ is the "ground truth", target/output
$p_i$ is the predicted probability of outcome

Example:
$t_0, t_1 = (0.3, 0.7)$
$p_0, p_1 = (0.2, 0.8)$

## Regularization
$\ell(w)$ in the past we use mean square error (regression), or cross-entropy loss

We can use L2 and L1 norm regularization, where L2 promotes more "smoother" boundaries and the L1 norm promotes sparsity. 

Reminder: L0 norm counts number of non zero functions, L1 is an approximation to L0 norm, but L0 is highly non convex, so we use L1.

Gradient descent is iterative. We can also try early-stopping, which has the same type of regularization effect as L2-norm (think prevent overfitting). 

## Learning Rates

## VGG

## Dropout
"Decision Trees"
- Bagging: take the average of multiple decision trees as our classifier

How we implement bagging is by removing some edges from the neural network randomly. 

When we train, we divide datasets into minibatches (32/64/128)

The ideas is that if my network depends on one weight it probably is overfitting.

## Channels
Channels is the "3rd" dimension of an image. For an RGB image of size 128, the number of channels is 3 and the dimensionality is 128x128x3.

One of the standard techniques to propagate information is to dercrease 1st and 2nd dimension but increase # of channels. 

## Applications
