{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = np.load(\"input.npy\")\n",
    "# output = np.load(\"output.npy\")\n",
    "# x = np.array([input**2, input]).T\n",
    "# y = output.reshape((output.shape[0], -1))\n",
    "\n",
    "# tts = train_test_split(x, y)\n",
    "# train_x = tts[0][0]\n",
    "# train_y = tts[2][0]\n",
    "# test_x = tts[1][0]\n",
    "# test_y = tts[3][0]\n",
    "\n",
    "# def gen_x(N, d):\n",
    "#     arr = np.random.randint(100, size=(N,d))\n",
    "#     return arr\n",
    "\n",
    "# def gen_y(x):\n",
    "#     s = np.sum(x, axis=1)\n",
    "#     boolean = x[:,0] / s\n",
    "#     return (boolean >= 0.5).astype(np.float32)\n",
    "\n",
    "def gen_x(N, d):\n",
    "    arr = np.random.randint(100, size=(N,d))\n",
    "    return arr\n",
    "\n",
    "def gen_y(x):\n",
    "    return (x[:, 0] >= 50).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Neural Networks\n",
    "## Problem 2.1 Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Derivation\n",
    "### Loss function gradient\n",
    "$\\ell(y_p, y_g) = -y_glog(y_p) - (1-y_g)log(1-y_p)$\n",
    "\n",
    "$y_p = \\sigma(w^Tx + w_0)$\n",
    "\n",
    "$\\nabla_w\\ell(y_p, y_g) = \\nabla_w[-y_glog(\\sigma(w^Tx+w_0)) - (1-y_g)log(1-\\sigma(w^Tx+w_0))]$\n",
    "\n",
    "$= -y_g(\\nabla_w\\sigma(w^Tx+w_0))(\\frac{1}{\\sigma(w^Tx+\\theta_0)}) - (1-y_g)(\\nabla_w\\sigma(w^Tx+w_0))(-\\frac{1}{1-\\sigma(w^Tx+w_0)})$\n",
    "\n",
    "$\\nabla_w\\ell = (-\\frac{y_g}{y_p} + \\frac{1-y_g}{1-y_p})\\nabla_wy_p$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear Function (sigmoid) gradient\n",
    "\n",
    "$y_p = \\sigma(w^Tx+w_0)$\n",
    "\n",
    "$\\nabla_wy_p = \\nabla_w\\sigma(w^Tx+w_0)$\n",
    "\n",
    "$u = w^Tx+w_0 = y_p$\n",
    "\n",
    "$\\sigma(u) = \\frac{1}{1+e^{-u}}$\n",
    "\n",
    "$\\nabla_wy_p = \\frac{\\delta\\sigma}{\\delta u}\\frac{\\delta u}{\\delta w}$\n",
    "\n",
    "$\\frac{\\delta\\sigma}{\\delta u} = (e^{-u})(\\frac{1}{(1+e^{-u})^2}) = \\frac{e^{-u}}{(1+e^{-u})^2} = \\frac{1}{1+e^{-u}}(\\frac{1+e^{-u}}{1+e^{-u}} - \\frac{1}{1+e^{-u}}) = \\sigma(u)(1-\\sigma(u))$\n",
    "\n",
    "$\\frac{\\delta u}{\\delta w} = x$\n",
    "\n",
    "$\\nabla_wy_p = \\sigma(y_p)(1-\\sigma(y_p))x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Update Step\n",
    "Since we have the gradient, we can use newton's method to approach the minimum loss given some weights for each update. Since we want to find the minimum, we subtract the gradient. We also add in a learning rate as a hyperparameter in order to not overshoot or end up in a local minima.\n",
    "\n",
    "$w_{n+1} = w_n - \\alpha\\nabla_w\\ell$\n",
    "\n",
    "so we have\n",
    "\n",
    "$w_{n+1} = w_n - \\alpha(-\\frac{y_g}{y_p} + \\frac{1-y_g}{1-y_p})\\nabla_wy_p = w_n - \\alpha[(-\\frac{y_g}{y_p} + \\frac{1-y_g}{1-y_p})][y_p(1-y_p)][x]$\n",
    "\n",
    "Since we are only using one point per update step, we would perform this step for every single epoch. If it was more than one point, then we can take the average of the gradients so it looks something like this:\n",
    "\n",
    "$w_{n+1} = w_n - \\alpha(\\frac{1}{N}\\sum_{i=1}^N-\\frac{y_{g,i}}{y_{g,i}} + \\frac{1-y_{g,i}}{1-y_{p,i}})\\nabla_wy_p$\n",
    "\n",
    "For $N$ number of points and where $y_{g,i}$ represents $y_g$ at point $i$ and $y_{p,i}$ represents $y_p$ at point $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.2 Backpropagation\n",
    "Let us start by defining our problem to be a $k$ layer neural network with input $x_k$ and output $y_{p,k}$, where $x_k$ is the input at layer $k$ and $y_{p,k}$ is the predicted output at layer $k$. We have labels $y$ and input into the neural net $x$ which correspond to $y_k$ and $x_0$ respectively.b We have weights $w$ and a non-linear function $\\sigma$. The first layer is right after inputing $y_p^0 = x_1$. Note that also $x^{i} = y_p^{i-1}$. We have $1, ..., k$ layers.\n",
    "\n",
    "$y_{p}^k = \\sigma^k(\\sum_i w^k_ix^{k-1}_i)$\n",
    "\n",
    "Thus at layer $L$ we have:\n",
    "$y_{p}^L = \\sigma^k(\\sum_i w^{L}_ix^{L-1}_i)$\n",
    "\n",
    "Let's have $u^k = \\sum_i w_i^kx^k_i$\n",
    "\n",
    "The Loss function is defined as:\n",
    "$\\ell(y_g, y_p^{L}) = \\ell(y, I(\\sum_i w_i^{L}x^{L-1}_i)) = \\ell(y, (\\sum_i w_i^{L}y_{p,i}^{L-1}))$\n",
    "\n",
    "The generalized formula of the gradient of the loss function with respect to $w$ is:\n",
    "\n",
    "$\\nabla_w\\ell = \\frac{\\delta \\ell}{\\delta \\sigma} \\frac{\\delta \\sigma}{\\delta u}\\frac{\\delta u}{\\delta w}$\n",
    "\n",
    "Note the gradient is a vector, and we can say that there are $j$ weights so we should denote each weight as $w_j$ giving us\n",
    "\n",
    "$\\nabla_{w_j}\\ell = \\frac{\\delta \\ell}{\\delta \\sigma} \\frac{\\delta \\sigma}{\\delta u}\\frac{\\delta u}{\\delta w_j}$\n",
    "\n",
    "And so then we can further generalize it to the $k$th layer by doing this:\n",
    "\n",
    "$\\nabla_{w_j}\\ell^k = \\frac{\\delta \\ell}{\\delta \\sigma^k} \\frac{\\delta \\sigma^k}{\\delta u^k}\\frac{\\delta u^k}{\\delta w^k_j}$\n",
    "\n",
    "In the proof, we will just solve for an aribtrary weight $w_j$ but note that we have to take its derivative with respect to each weight and that is what $w_j$ represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful equations we will use in the future:\n",
    "\n",
    "$\\nabla_wy_p = \\frac{\\delta\\sigma}{\\delta u}\\frac{\\delta u}{\\delta w}$\n",
    "\n",
    "$\\frac{\\delta\\sigma}{\\delta u} = (e^{-u})(\\frac{1}{(1+e^{-u})^2}) = \\frac{e^{-u}}{(1+e^{-u})^2} = \\frac{1}{1+e^{-u}}(\\frac{1+e^{-u}}{1+e^{-u}} - \\frac{1}{1+e^{-u}}) = \\sigma(u)(1-\\sigma(u))$\n",
    "\n",
    "$\\frac{\\delta u}{\\delta w} = x$\n",
    "\n",
    "$\\nabla_wy_p = \\sigma(y_p)(1-\\sigma(y_p))x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our last layer is not non-linear as we are interested in regression, we have:\n",
    "\n",
    "$y_p^L = \\sum_i w_i^Lx_i^{L-1}$\n",
    "\n",
    "and our loss function is $L_2$, so it is given by:\n",
    "\n",
    "$\\ell(y_g, y_p^L) = (y_g - y_p^L)^2$\n",
    "\n",
    "Now we use our chain rule above and get:\n",
    "\n",
    "$\\nabla_w\\ell^L = 2*(y_g - y_p^L) \\frac{\\delta y_p}{\\delta w} = 2*(y_g - \\sum_iw_i^Lx_i^{L-1})\\frac{\\delta y_p}{\\delta w_j} = 2*(y_g - \\sum_iw_i^Lx_i^{L-1})(-x^{L-1})$\n",
    "\n",
    "Now for some given layer $k$, we have:\n",
    "\n",
    "$\\frac{\\delta \\ell}{\\delta w^k} = \\frac{\\delta \\ell^k}{\\delta u^k}\\frac{\\delta u^k}{\\delta w^k}$\n",
    "\n",
    "$u^k = \\sum_i w_i^k x^{k} = \\sum_i w_i^k y_p^{k-1}$\n",
    "\n",
    "$\\frac{\\delta u^k}{\\delta w^k} = y_p^{k-1}$\n",
    "\n",
    "$\\frac{\\delta \\ell}{\\delta u^k} = \\sum_i \\frac{\\delta \\ell}{\\delta u_i^{k+1}}\\frac{\\delta u_i^{k+1}}{\\delta u^k}$\n",
    "\n",
    "$u_i^{k+1} = \\sum_s w_{s,i}^{k+1}y_{p,s}^{k} = \\sum_s w_{s,i}^{k+1}\\sigma(u^{k})$\n",
    "\n",
    "$\\frac{\\delta u_i^{k+1}}{\\delta u^k} = \\sigma'(u^k) w_{ji}^{k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta \\ell}{\\delta u^k} = \\sum_i \\frac{\\delta \\ell}{\\delta u_i^{k+1}}\\sigma'(u^k_j)w_ji^{k+1}$\n",
    "\n",
    "$ = \\sigma'(u^k_j) \\sum_i \\frac{\\delta \\ell}{\\delta u_i^{k+1}}w_ji^{k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the gradient of the $k$ th layer relative to the $k+1$ layer, we have base case $k+1 = L$ so we can do back propagation. Here are the important equation takeaways.\n",
    "\n",
    "For layer $L$ as the last layer, the gradient is given by:\n",
    "\n",
    "$\\frac{\\delta \\ell(w)}{\\delta w^L} = 2*(y_g - \\sum_i w^Lx^{L-1})(-x^{L-1})$\n",
    "\n",
    "$\\frac{\\delta \\ell(w)}{\\delta u^L} = 2*(y_g - \\sum_i w^Lx^{L-1})$\n",
    "\n",
    "For layer $k$ as the any layer from $1..L-1$, the gradient to update the weights is given by:\n",
    "\n",
    "$\\frac{\\delta \\ell(w)}{\\delta w^k} = \\sigma(u^k)(1-\\sigma(u^k)) (\\sum_i \\frac{\\delta \\ell(w)}{\\delta u_i^{k+1}}w_{ji}^{k+1}) y_p^{k-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feel free to ignore all code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_entropy_loss(w, x, y_g):\n",
    "#     # y_g is {-1, 1}\n",
    "#     return -y_g*np.log(w*x) - (1-y_g)*np.log(1-w*x)\n",
    "\n",
    "def cross_entropy_loss(y_p, y_g):\n",
    "    # y_g is {-1, 1}\n",
    "    return -y_g*np.log(y_p) - (1-y_g)*np.log(1-y_p)\n",
    "\n",
    "def cross_entropy_deriv(w, x, y_g, predict):\n",
    "    y_p = predict(w, x)\n",
    "    return -(y_g / y_p) + (1-y_g) / (1-y_p)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # assert type(x) == np.ndarray\n",
    "    denominator = 1 + np.exp(-x)\n",
    "    return 1 / denominator\n",
    "\n",
    "def sgn(x):\n",
    "    return np.sign(x)\n",
    "\n",
    "def hinge_loss(w, x, y_g, predict):\n",
    "    y_p = predict(w, x)\n",
    "    return np.sum(np.maximum(0, 1-y_p*y_g))\n",
    "\n",
    "def hinge_deriv(w, x, y_g, predict):\n",
    "    y_p = predict(w, x)\n",
    "    if y_p*y_g < 1:\n",
    "        return np.sum(-y_g*x)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Currently only supports a very specific implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y, nlf, ell, dell, loss, lr=0.01, tol=0.01):\n",
    "        assert (x.shape[0], 1) == (y.shape[0], 1), \"Make sure your matrices are (N,d) and (N,1)\" # (Class, dim)\n",
    "        # assert len(x.shape) == 2, \"Make sure x is 2D. If not do reshape((N, -1))\"\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "\n",
    "        self.nlf_ = nlf # Non linear function (sigmoid)\n",
    "        self.ell_ = ell\n",
    "        self.dell_ = dell # Derivative of Loss Function\n",
    "        self.loss_ = loss\n",
    "        self.lr_ = lr\n",
    "        self.tol_ = tol\n",
    "        self.num_points_ = x.shape[0] # N\n",
    "        self.dim_ = x.shape[1] + 1 # Add one to dimension for bias, d\n",
    "        self.w_ = np.random.rand(self.dim_) # Initialize random weights [0, 1)\n",
    "        self.w_ /= la.norm(self.w_)\n",
    "        self.x_ = x.copy()# Append a one at beginning for bias\n",
    "        # np.insert(x.reshape((self.num_points_, -1)), 0, np.ones(self.num_points_), axis=1) \n",
    "        self.y_g = y.T.flatten().copy()\n",
    "\n",
    "    def mini_batch_gradient_descent(self, lr=0.01, batch_size=5, epochs=25):\n",
    "        error = []\n",
    "        for _ in range(epochs):\n",
    "            idx = np.random.randint(self.num_points_, size=batch_size)\n",
    "            grad = 0\n",
    "            err = 0\n",
    "            for i in idx:\n",
    "                err += self.ell_(self.w_, self.x_[i], self.y_g[i], self.predict)\n",
    "                grad += self.dell_(self.w_, self.x_[i], self.y_g[i], self.predict)\n",
    "            grad /= idx.shape[0]\n",
    "            err /= idx.shape[0]\n",
    "            error.append((_, err))\n",
    "            self.w_ = self.w_ - lr * grad\n",
    "        return self.w_, error\n",
    "    \n",
    "    def predict(self, w, x):\n",
    "        \"\"\"Performs feed forward with weight summation and non-linear activation\n",
    "        \"\"\"\n",
    "        # assert row.shape[0] == self.dim_, str(row.shape) + \" \" + str(self.dim_)\n",
    "        mean = np.average(x)\n",
    "        std = np.std(x)\n",
    "        normalized_x = (x - mean) / std\n",
    "        return self.nlf_(np.sum(w[1:] * normalized_x) + w[0])\n",
    "        \n",
    "    def prediction(self, x):\n",
    "        return self.predict(self.w_, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = gen_x(100, 3)\n",
    "train_y = gen_y(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.4),\n",
       " (1, 0.8),\n",
       " (2, 0.6),\n",
       " (3, 0.2),\n",
       " (4, 0.8),\n",
       " (5, 0.6),\n",
       " (6, 0.0),\n",
       " (7, 0.8),\n",
       " (8, 0.4),\n",
       " (9, 0.4),\n",
       " (10, 0.6),\n",
       " (11, 0.6),\n",
       " (12, 0.6),\n",
       " (13, 0.4),\n",
       " (14, 0.4),\n",
       " (15, 0.6),\n",
       " (16, 0.2),\n",
       " (17, 0.6),\n",
       " (18, 0.4),\n",
       " (19, 0.2),\n",
       " (20, 0.4),\n",
       " (21, 0.8),\n",
       " (22, 0.6),\n",
       " (23, 0.8),\n",
       " (24, 0.4),\n",
       " (25, 0.6),\n",
       " (26, 0.8),\n",
       " (27, 0.4),\n",
       " (28, 0.2),\n",
       " (29, 0.6),\n",
       " (30, 0.6),\n",
       " (31, 0.8),\n",
       " (32, 0.4),\n",
       " (33, 0.6),\n",
       " (34, 0.4),\n",
       " (35, 0.6),\n",
       " (36, 0.4),\n",
       " (37, 0.0),\n",
       " (38, 0.0),\n",
       " (39, 0.2),\n",
       " (40, 0.4),\n",
       " (41, 0.4),\n",
       " (42, 0.4),\n",
       " (43, 0.4),\n",
       " (44, 0.2),\n",
       " (45, 0.6),\n",
       " (46, 0.6),\n",
       " (47, 0.8),\n",
       " (48, 0.6),\n",
       " (49, 0.8),\n",
       " (50, 0.6),\n",
       " (51, 0.8),\n",
       " (52, 0.6),\n",
       " (53, 0.4),\n",
       " (54, 0.4),\n",
       " (55, 0.8),\n",
       " (56, 0.8),\n",
       " (57, 0.4),\n",
       " (58, 0.6),\n",
       " (59, 0.6),\n",
       " (60, 0.2),\n",
       " (61, 0.4),\n",
       " (62, 0.2),\n",
       " (63, 0.6),\n",
       " (64, 0.4),\n",
       " (65, 0.6),\n",
       " (66, 0.4),\n",
       " (67, 0.6),\n",
       " (68, 0.2),\n",
       " (69, 0.4),\n",
       " (70, 0.4),\n",
       " (71, 0.4),\n",
       " (72, 0.4),\n",
       " (73, 0.6),\n",
       " (74, 0.2),\n",
       " (75, 0.6),\n",
       " (76, 0.4),\n",
       " (77, 0.8),\n",
       " (78, 0.2),\n",
       " (79, 0.6),\n",
       " (80, 0.6),\n",
       " (81, 0.4),\n",
       " (82, 0.8),\n",
       " (83, 0.2),\n",
       " (84, 0.2),\n",
       " (85, 0.4),\n",
       " (86, 0.4),\n",
       " (87, 0.0),\n",
       " (88, 0.6),\n",
       " (89, 0.2),\n",
       " (90, 0.0),\n",
       " (91, 0.6),\n",
       " (92, 0.8),\n",
       " (93, 0.4),\n",
       " (94, 0.2),\n",
       " (95, 0.4),\n",
       " (96, 0.4),\n",
       " (97, 0.6),\n",
       " (98, 0.6),\n",
       " (99, 0.6)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p = Perceptron(train_x, train_y, sigmoid, cross_entropy_loss, cross_entropy_deriv, cross_entropy_loss)\n",
    "p = Perceptron(train_x, train_y, sgn, hinge_loss, hinge_deriv, cross_entropy_loss)\n",
    "w, error = p.mini_batch_gradient_descent(batch_size=5, epochs=100)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 1.0, array([1.], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = gen_x(1, 3)\n",
    "test_y = gen_y(test_x)\n",
    "test_x[0,0], p.prediction(test_x), test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f0ac818ec00c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_entropy_deriv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'loss'"
     ]
    }
   ],
   "source": [
    "p = Perceptron(train_x, train_y, sigmoid, cross_entropy_deriv, cross_entropy_loss)\n",
    "y_p = p.feed_forward()\n",
    "y_p[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_deriv(y_p[:5], train_y[:5].flatten())\n",
    "# # np.apply_along_axis(cross_entropy_deriv, 0, )\n",
    "# # train_y[:5].T\n",
    "# y_p.shape, train_y.shape\n",
    "idx = np.random.randint(y_p.shape[0], size=3)\n",
    "y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e018137ffda6e84bad91b13a13082b3438e14729982a434c35ce4ecabfe2410"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
