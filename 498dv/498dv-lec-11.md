# Decision Trees
Example: Is Green St. going to be busy today?

Training set $\rightarrow$ 1000 days, busy or not busy

Features: Term (Summer/Fall/Spring), weather (Sunny, rain, snow, thunder), school states (session or break), day of week

We can construct a decision tree where each node is a decision on a feature that we pick.

Pros:
- Interpretability
- Complex decision boundaries

What functions can a decision tree represent?
- Represent all functions
- Boolean functions

## Learning Decision Trees
1. For any boolean function, there is a decision tree
2. "Simple" decision trees $\rightarrow$ NP complete

## Greedy/Recursive Algorithm
1. Pick a feature
   1. This splits the dataset into two (Yes/No) categories
2. Recursively pick a feature (which splits the dataset) at each node

## Criteria for Splitting
### Probability
We want to choose criteria that will heavily be against or for a class given the feature. We don't want want uniform distributions (ie. $p(Y=0) = 0.5$ and $p(Y=1) = 0.5$). Here we have high entropy because we do not know what is going on. We want to decrease that.

Things to keep in mind
1. Uniform distributions are bad
2. We want "peaky" distributions

## Entropy
$H(y) = -\sum_{i=1}^k p(Y=y_i)log_2 (p(Y=y_i))$

With $y_1 = 0.5$ and $y_2 = 0.5$, we have $H(y) = 1$.

Entropy is a measure of randomness. If we had a coin where it always lands head, we would have an entropy of 0. If it is 50/50, we have max entropy at 1. It is also defined as the "least no. of digits required to represent the outcome of your experiments"

We pick the feature by minimizing this entropy.

## Information Gain
$H(y|x) = -\sum_{j=1}^vp(x=x_j)\sum_{i=i}^{k}p(y=y_i|x=x_j)log_2(pY=y_i|x=x_j))$

$IG(y; x) = H(y) - H(y|x)$

$H(y|x)$ calculates the largest reduction in entropy. The idea is how much of the entropy reduction does picking $x$ lead to?

### Information Gain Example
Conditional Entropy:
Let's do dataset given by mathematical boolean function OR.

| x_1 | x_2     |  y     |
| :---|:----:   |   ---: |
| 0   | 0       | 0      |
| 0   | 1       | 1      |
| 1   | 0       | 1      |
| 1   | 1       | 1      |
| 1   | 1       | 1      |
| 0   | 1       | 1      |

$P(Y=1) = \frac{5}{6}$, $P(Y=0) = \frac{1}{6}$

$H(y) = 0.65$

$H(y|x^{(i)}) = (p(x^{(i)}=0)\{-p(Y=0|x^{(i)}=0)log(...) - p(Y=1|x^{(i)}=0)log(...)\}) + (p(x^{(i)}=1)\{-p(Y=0|x^{(i)}=1)log(...) - p(Y=1|x^{(i)}=1)log(...)\}) = \frac{2}{6}$

## Back to learning
Recursively create nodes and split according to highest information gain at each level.

## Stopping Criterion
Ideas:
1. IG = 0
2. all examples have same label
3. all examples have same features

### Idea 1: Using XOR for IG = 0
If we use XOR here, at each level of the tree (given $x_1, x_2$ and the standard boolean truth table), we have IG = 0.5 for all nodes. Therefore, it is unhelpful and impossible to use IG = 0 as a criteria for stopping. Instead we should use Idea 2 or Idea 3 instead.

## Overfitting
If we let the tree grow to maximum depth, it could be such that each leaf node contains only one data point and overfits while achieving 100% accuracy on test data.

Avoid overfitting:
- Limit decision tree size (hyperparameter)
- Pruning
  - If I remove a node and my accuracy goes up, I am overfitting
- Minimum samples per node

## Generalize to Real Valued Inputs (ie. continuous)
We need:
1. Threshold
2. Feature

This allows us to determine (through inequality) which feature/threshold pair leads to maximum information gain.

## Picking the best threshold (not covered)

## Ensemble Methods
Train $N$ decision trees that look slightly different. But we need to introduce randomness

How to get randomness:
1. Subsample training set (bagging)
2. Random forests $\rightarrow$ each split, consider a subset of your features (as opposed to all of them)

