Post/email on CW if we have recommendations for guest speakers.

# Perception
Input: $x \in \mathbb{R}^D$, ie. $x_1, x_2, ..., x_D$
$x_{in}$ where $i$ is the dimension, and $n$ is the training sample
Have $y \in \{-1, 1\}$

We have weights into some node, which we apply a non-linear filter over it.
For example, we can have non-linear function (nlf) where it is the sign of the summation.
$\hat{y} = sgn(\Sigma_iw_ix_i)$

We want loss function to be differentiable, that we can find the gradient.
Misclassification loss: $\hat{y} = y \rightarrow 0$, $\hat{y} \neq y \rightarrow 1$
^ however, it is hard to compute gradient

We can just look at the thing inside the nlf such that $\Sigma_iw_ix_i$ has the same sign as $y$ & it's large, then it's good for me. What that means is that for all samples I missclassifiy, I want to minimize $\Sigma_{n\in\bar{M}}w^Tx_ny_n = \Sigma_ny_n\Sigma_iw_ix_{in}$ (where $M$ is correct classification, and $\bar{M}$ is misclassification). Intuition is we want to bring our perception as close to the boundary as possible.

Perception loss:
loss function, $\ell(w) = \Sigma_{n\in \bar{M}}w^Tx_ny_n$

## Gradient Descent
$\nabla_w\ell(w) = \frac{d\ell(w)}{dw_i} = \frac{d}{dw_i}(\Sigma_nw^Tx_ny_n) = -\Sigma x_{in}y_n$

start: $w^0$
update: $w^{t+1} = w^t-\alpha\nabla_w\ell(w)$, where $\alpha$ is the learning rate

Convergence is guaranteed for this classification problem, but there (we don't think) is not a closed form solution.

## Convergence
Convergence of gradient descent is guaranteed. 

# Neural Networks
We have multiple dimensional input layer $x_1$ to $x_D$, output layer $y_1$ to $y_M$, and hidden layer(s) $z_1$ to $z_h$, connected with linear weights.

Define $w^k_{ij}$ to be from nodes in layer $i$ to $j$. 
$z_i = \Sigma_{i=1}^Dw_{ji}x_i$

We have just have linear functions and non-linearlity, the composition of all these weights will just result in another linear function, so we need a non-linearlity in order to make it useful.

Define $h$ has a non-linear operation, and we will have
$z_i = h(\Sigma_{i=1}^Dw_{ji}x_i)$

## Non-linearities
### Last Layer
#### Classification:
Here, we care about classification, so we can use something like $sgn$, but we can't differentiate it, so we use the $sigmoid$ function instead.

$sigmoid = \sigma(x) = \frac{1}{1+e^{-x}}$

#### Multi-class classification:
Here we want a one-hot encoding for output class, so we want to do something like $max$, but it's non-differentiable, so we use $softmax$ function.

$softmax(y_k) = \frac{e^{y_k}}{\Sigma_{m=1}^{M}(e^{y_m})}$

#### Another popular one, tanh
$tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}$

## Gradients
Define $z^{(k)}_{h}$ where $k$ is the $k^{th}$ layer, so $z^{(0)} = x$

$z^{(k)}_j = h(\Sigma_{i=1}^{h^k_{ij}}w^k_{i}z^{k-1}_{i})$
where we set the inside $\Sigma_{i=1}^{h^k_{i}}w^k_{i}z^{k-1}_ij$ as $a_j^k$

$z_L = h(\Sigma_i w^k_{i}z^{k-1}_i)$

For regression, let's have loss function be the L2 norm
$\ell(w) = (y - z^L)^2$

$\nabla_w\ell(w) = 2(y-z_j)(-z_j)$...